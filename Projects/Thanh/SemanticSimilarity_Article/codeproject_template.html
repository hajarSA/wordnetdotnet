<!-- saved from url=(0022)http://internet.e-mail -->
<!--------------------------------------------------------------------------->  
<!--                           INTRODUCTION                                

 The Code Project article submission template (HTML version)

Using this template will help us post your article sooner. To use, just 
follow the 3 easy steps below:
 
     1. Fill in the article description details
     2. Add links to your images and downloads
     3. Include the main article text

That's all there is to it! All formatting will be done by our submission
scripts and style sheets. 

-->  
<!--------------------------------------------------------------------------->  
<!--                        IGNORE THIS SECTION                            -->
<html>
<head>
<title>The Code Project</title>
<Style>
BODY, P, TD { font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 10pt }
h2, h3, h4, h5 { color: #ff9900; font-weight: bold; }
H2 { font-size: 13pt; }
H3 { font-size: 12pt; }
H4 { font-size: 10pt; color: black; }
PRE { BACKGROUND-COLOR: #FBEDBB; FONT-FAMILY: "Courier New", Courier, mono; WHITE-SPACE: pre; }
CODE { COLOR: #990000; FONT-FAMILY: "Courier New", Courier, mono; }
.bodyline	{ background-color: #FFFFFF; border: 1px #98AAB1 solid; }

.forumline	{ background-color: #FFFFFF; border: 2px #006699 solid; }

td.row1	{ background-color: #EFEFEF; }
.postbody { font-size : 12px; line-height: 18px}
td.row2	{ background-color: #DEE3E7; }
</style>
<link rel="stylesheet" type=text/css href="http://www.codeproject.com/styles/global.css">
</head>
<body bgcolor="#FFFFFF" color=#000000>
<!--------------------------------------------------------------------------->  


<!-------------------------------     STEP 1      --------------------------->
<!--  Fill in the details (CodeProject will reformat this section for you) -->

<!-------------------------------     STEP 2      --------------------------->
<!--  Include download and sample image information.                       --> 

<ul class=download>
<li><a href="Article_demo.zip">Download demo project - XXX Kb </a></li>
<li><a href="Article_src.zip">Download source - XXX Kb</a></li>
</ul>


<!-------------------------------     STEP 3      --------------------------->
<!--  Add the article text. Please use simple formatting (<h2>, <p> etc)   --> 

<h2><font size="4">Introduction</font></h2>

<font FACE="CMR12">
<p ALIGN="LEFT">In the previous article, we presented an approach for capturing 
similarity between words that was concerned with the syntactic similarity of two 
strings. Today we are back to discuss another approach that is more concerned 
with the 
meaning of words. Semantic similarity is a confidence score that reflects 
the semantic relation between the meanings of two sentence. It is difficult to 
gain a high accuracy score because exact semantic meanings are only 
completely understood in a particular context.<br>
<br>
The goals of this paper are to:</p>
<ul>
  <li>
<p ALIGN="LEFT">Present to you some dictionary-based algorithms to capture semantic 
similarity between two sentences, which is heavily based on the WordNet semantic 
dictionary. </p>
  </li>
  <li>
<p ALIGN="LEFT">Encourage you to work with the interesting topic of NLP.</p>
  </li>
</ul>
</font>

<h2><font size="4">Groundwork</font></h2>

<font FACE="CMR12">
<p ALIGN="LEFT">Fortunately, there are a lot of previous scientific articles on 
semantic similarity and semantic relatedness measurement developed in the 
context of information integration, information retrieval, etc. </p>
</font>

<p><b>WordNet </b></p>
<p>WordNet is a lexical database which is available online and provides a large 
repository of English lexical items. There is a multilingual WordNet for 
European languages which are structured in the same way as the American WordNet.<br>
<br>
WordNet was designed to establish connections between four types of Parts of Speech 
(POS) - noun, verb, adjective, and adverb. The smallest unit in 
WordNet is synset, which represents a specific meaning of a word. It includes 
the word, its explanation, and its synonyms. The specific meaning 
of one word under one type of POS is called a sense. Each sense of a word is in 
a different synset. Synsets are equivalent to senses = structures containing 
sets of terms with synonymous meanings. Each synset has a gloss that defines the 
concept it represents. For example the words night, nighttime and dark 
constitute a single synset that has the following gloss: the time after sunset 
and before sunrise while it is dark outside. Synsets are connected to one 
another through explicit semantic relations. Some of these relations (hypernym, 
hyponym for nouns and hypernym and troponym for verbs) constitute is-a-kind-of (holonymy) 
and is-a-part-of (meronymy for nouns) hierarchies. <br>
<br>
For example, tree is a kind 
of plant, tree is a hyponym of plant and plant is hypernym of tree. Analogously, 
trunk is a part of tree and we have that trunk is a meronym of tree and tree is 
holonym of trunk. For one word and one type of POS, if there are more than one 
sense, WordNet organizes them in the order of the most frequently used to the 
least frequently used (Semcor).</p>
<p><b>WordNet .NET</b></p>
<p>Malcolm Crowe and Troy Simpson have developed an open-source WordNet.Net....</p>

<h2><font size="4">Semantic similarity between sentences</font></h2>

<p>Given two sentences, the measurement is to determine how similar the meaning 
of two sentences are. The higher the score the 
more similar the meaning of the two sentences.</p>
<p>Steps for computing semantic similarity between two sentences:</p>
<ul>
  <li>First each sentence is partitioned into a list of tokens.</li>
  <li>Part-of-speech disambiguation or tagging.</li>
  <li>Stemming words.</li>
  <li>Find the most appropriate sense for every word in a 
  sentence (Word Sense Disambiguation).</li>
  <li>Finally, compute the similarity of the sentences based on the similarity 
  of the pairs of words. </li>
</ul>

<h2>Tokenization</h2>
<p>Each sentence is partitioned into list of words and we remove the stop 
words.<font SIZE="2"> Stop words are frequently occurring, insignificant words 
that appear in a database record, article or web page , etc.</font></p>

<h2>Tagging part of speech (*) </h2>
<p>This task is to identify the correct part of speech (POS - like noun, verb, 
pronoun, adverb ...) of each word in the sentence. The algorithm 
takes a sentence as input and a&nbsp;specified tag set (a finite list of POS tags). The 
output is a single best POS tag for each word. There are two types of taggers: 
the first one attaches syntactic roles to each word (subject, object, ..) and 
the second 
one attaches only functional roles (noun, verb, ...). There is a lot of work 
that has 
been done on POS tagging. The tagger can be classified as rule-based or stochastic. 
Rule-based taggers use hand written rules to disambiguate tag ambiguity. An example of rule-based 
tagging is 
Brill's tagger (Eric Brill algorithm) .&nbsp; Stochastic taggers resolve tagging 
ambiguities by using a training corpus to compute the probability of a given 
word having a given tag in a given context. For example: tagger using the Hidden 
Markov Model, Maximize likelihood.</p>

<h2>Stemming word (*)</h2>
<p>We use the Porter stemming algorithm.&nbsp;Porter stemming is a process of 
removing the common morphological and inflexional endings of words. It can be 
thought of as a lexicon finite state transducer with the following steps:<br>
Surface form -&gt; split word into possible morphemes -&gt;&nbsp; getting intermediate 
form -&gt; map stems to categories and affixes to meaning -&gt; underlying form.</p>

<p>i.e : foxes -&gt; fox + s -&gt; fox.</p>

<p>(*) This work is not available in this release. </p>

<h2>Semantic relatedness and <b>Word sense disambiguation (WSD)</b></h2>

<p>As you are already aware, a word can have more than one sense that
<font size="2">can lead to ambiguity. <br>
<br>
For example : the word “interest&quot; has different meaning in the two following 
contexts:<br>
&nbsp;&nbsp; •“Interest” from a bank.<br>
&nbsp;&nbsp; •“Interest” in a subject.</font></p>
<font FACE="Times New Roman" SIZE="3">
<p ALIGN="LEFT"><b>WSD with original Micheal Lesk Algorithm</b></p>
</font>
<p>Disambiguation is the process of finding out the most appropriate<font size="2"> 
sense of a word that is used in a given sentence. The Lesk algorithm uses dictionary definitions 
(gloss) to disambiguate a polysemous word in a 
sentence context. The major objective of his idea is to count the number of words that are 
shared between two 
glosses. The more overlapping words, the more 
related the senses are.</font></p>
<font FACE="Times New Roman" SIZE="3">
<p ALIGN="LEFT">To disambiguate a word, the gloss of each of its senses is 
compared to the glosses of every other word in a phrase. A word is assigned 
to the sense whose gloss shares the largest number of words in common with the 
glosses of the other words. </p>
<p ALIGN="LEFT"><span class="postbody">For example: In performing disambiguation 
for the &quot;pine cone&quot; phrasal, according to&nbsp; the </span>Oxford Advanced 
Learner’s Dictionary,<span class="postbody"> the word &quot;pine&quot; has two senses: </span>
</p>
<ul>
  <li>
  <p ALIGN="LEFT"><span class="postbody">sense 1: kind of <i>evergreen tree </i>
  with needle–shaped leaves</span></li>
  <li>
  <p ALIGN="LEFT"><span class="postbody">sense 2: waste away through sorrow or 
  illness.</span></li>
</ul>
<p ALIGN="LEFT"><span class="postbody">the word &quot;cone&quot; has 3 senses: </span></p>
<ul>
  <li>
  <p ALIGN="LEFT"><span class="postbody">sense 1: solid body which narrows to a 
  point </span></li>
  <li>
  <p ALIGN="LEFT"><span class="postbody">sense 2: something of this shape 
  whether solid or hollow </span></li>
  <li>
  <p ALIGN="LEFT"><span class="postbody">sense 3: fruit of certain
  <span style="font-style: italic">evergreen tree</span> </span></li>
</ul>
<p ALIGN="LEFT"><span class="postbody">=&gt; By comparing each of the two gloss senses of the 
word &quot;pine&quot; with each of the three senses of the word &quot;cone&quot;, it is found that 
the words &quot;<span style="font-style: italic">evergreen tree</span>&quot; occurs in one 
sense in each of the two words. So these two senses are then declared to be the 
most appropriate senses when the words &quot;pine&quot; and &quot;cone&quot; are used together.
<br>
<br>
</span>The original Lesk algorithm begins anew for each word and does not 
utilize the senses it previously assigned. This greedy method does not always 
work effectively. Therefore if the computational time is not critical we should 
think of optimal sense combination by applying local search techniques such 
as Tabu or Beam. The major idea behind such methods is to reduce the 
search space by applying several heuristic techniques. The Beam searcher limits its 
attention to only <i>k </i>most promising candidates at each stage of the search 
process, where k is a predefined number. Tabu is a method in which a fundamental 
role is played by keeping track of features of previously visited solutions,<font FACE="Palatino-Roman~19" SIZE="3">
</font>which makes use of memory in a more efficient manner. It keeps a list of moves that 
are forbidden to be performed in order to prevent cycling.<span class="postbody"><br>
<br>
</span><b>The adapted Micheal Lesk Algorithm</b></p>
</font>
<p><font size="2">The original Lesk used the gloss of a word and 
is restricted on the overlap scoring mechanism. In this section, we introduce 
an adapted version of the algorithm with some improvements to overcome the limitations:</font></p>

<ul>
  <li>Access<font size="2"> a dictionary with senses arranged in a 
  hierarchical order (WordNet). This extended version uses not only the 
  gloss/definition of the synset but also considers the meaning of related 
  words.</font></li>
  <li><font size="2">Apply a new scoring mechanism to measure gloss overlap 
  that gives a more accurate score than the original Lesk bag of words counter. </font>
  </li>
</ul>
<p><font size="2">To disambiguate each word in a sentence that has N words, we 
call each word to be disambiguated as a target word. The algorithm is described by 
the following 
steps:</font></p>

<p>1.<font size="2"> Select a context: optimises computational time 
so if N is long, we will define K 
context around the target word (or k-nearest neighbor) as the sequence of words starting K words to the 
left of the target word and ending K words to the right. This will reduce the 
computational space that decreases the processing time. For example: If K is four, there 
will be two words to the left of the target word and two words to the right. </font></p>

<p>2. For each word in the selected context, we<font size="2"> look up and list 
all the possible senses of both POS (part of speech) noun and verb. </font></p>

<p>3. For each sense of a word (WordSense), we list the following relations 
(example with pine and cone):</p>

<ul>
  <li>Its own gloss/definition that includes 
example texts that WordNet provides to the glosses.&nbsp; </li>
  <li>The gloss of the synsets that are connected to it through the hypernym 
  relations. If there is more than one hypernym for a word sense, then the 
  glosses for each hypernym are concatenated into only single one gloss string 
  (*).</li>
  <li>The gloss of the synsets that are connected to it through the hyponym 
  relations. (*)</li>
  <li>The gloss of the synsets that are connected to it through the meronym 
  relations. (*)</li>
  <li>The gloss of the synsets that are connected to it through the troponym 
  relations. (*)<p>(*) All of them are applied the same rule.</p>

  </li>
</ul>

<p>4. Combine all possible gloss pairs that are archived at previous steps and 
compute the relatedness by searching for overlap. The overall score is the sum 
of the scores for each relation pair. <br>
<br>
When computing the relatedness between two synsets s1 and s2, the pair 
hype-hype means the gloss for the hypernym of s1 is compared to gloss for the 
hypernym of s2. The pair hype-hypo means that means the gloss for the hypernym 
of s1 is compared to gloss for the hyponym of s2.<br>
<br>
-&gt; OverallScore(s1, s2)= Score(hype(s1)-hypo(s2)) + Score(gloss(s1)-hypo(s2)) + 
Score(hype(s1)-gloss(s2))...<br>
( OverallScore(s1, s2) is also equivalent to OverallScore(s2, s1) ).<br>
<br>
In the example of &quot;pine cone&quot;, there are 3 senses of pine and 6 senses of cone, 
so we can 
have a total of 18 possible combinations. One of them is the right one.</p>

<p>To score the overlap we use a new scoring mechanism (1) that differentiates 
between N-single words and N-consecutive word overlaps and effectively treats 
each gloss as a bag of words. <font size="2">It is based on ZipF's Law, which 
says that the length of words is inversely proportional to their usage. The 
shortest words are those which are used more, the longest ones are used less.</font>
<br>
<br>
Measuring overlaps between two strings is reduced to solve the problem of finding 
the 
longest common sub-string with maximal consecutives.
Each overlap which contains N consecutive words, 
contributes N^2 to the score of gloss sense combination. <br>
<br>
For example:&nbsp;
an 
overlap &quot;ABC&quot; has score 3^2=9 and two single overlaps &quot;AB&quot; and &quot;C&quot; has score
2^2 + 1^1=5. </p>

<p>5. Once each combination has been scored, we pick up the sense that has the highest 
score to be the most appropriate sense for the target word in the selected context space. Hopefully the output not only gives us the most appropriate sense but also the 
associated part of speech for a word.</p>

<p>
<span class="postbody">If you intend to work with this topic, you should refer to 
the measurements of Hirst-St.Onge&nbsp; which based on finding 
the lexical chains between synsets.</span></p>

<h2>Semantic similarity between two synsets</h2>

<p>The above method allows us to find the most appropriate sense for each word 
in a sentence. To compute the similarity between two sentences, we base the 
semantic similarity between word senses. We capture semantic similarity between two word 
senses based on the path length similarity.  </p>

<p>In WordNet, each part of speech words (nouns/verbs...) are organized into 
taxonomies where each node is a set of synonyms (synset) represented in one sense. If 
a word has more than one sense, it will appear in multiple synsets at various 
locations in the taxonomy. WordNet defines relations between synsets and 
relations between word senses. A relation between synsets is a semantic relation, 
and a relation between word senses is a lexical relation. The difference is that 
lexical relations are relations between members of two different synsets, but 
semantic relations are relations between two whole synsets.<br>
&nbsp;<br>
For instance: </p>

<ul>
  <li>Semantic 
relations are hypernym, hyponym, holonym , etc.&nbsp; </li>
  <li>Lexical relations are antonym relation and the derived form relation. 
  </li>
</ul>

<p>Using the example, the antonym of the tenth sense of the noun light (light#n#10) in WordNet 
is the first sense of the noun dark (dark#n#1). The synset to which it belongs is 
{light#n#10, lighting#n#1}. Clearly it makes sense that light#n#10 is an antonym 
of dark#n#1, but lighting#n#1 is not an antonym of dark#n#1; therefore the 
antonym relation needs to be a lexical relation, not a semantic relation.</p>

<p>Semantic similarity is a special case of semantic relatedness where we only 
consider the IS-A relationship.</p>

<p>
<b>The Path Length based similarity</b></p>

<p>To measure the semantic similarity between two synsets we use 
hyponym/hypernym (or is-a relations). Due to the limitation to is-a hierarchies, 
we only work with &quot;noun-noun&quot;, and &quot;verb-verb&quot; parts of speech.</p>

<p>A simple way to measure the semantic similarity between two synsets is to 
treat the taxonomy as an undirected graph and measure the distance between them 
in WordNet. Said P. Resnik : &quot;The shorter the path from one node to another, the 
more similar they are&quot;. Note that path length is measured in nodes/verteces 
rather than links/edges. The length of the path between two members of the same synsets is 1 (synonym relations). </p>

<p>This figure shows an example of the hyponym taxonomy in WordNet used for path length 
similarity measurement:</p>

<p><img border="0" src="CarFork_Taxonomy.png" width="587" height="304"></p>

<p>In the above figure, we observe that the length between car and auto is 1, 
car and truck is 3, car and bicycle is 4, car and fork is 12.</p>

<p>A shared parent of two synsets is known as a sub-sumer. The least common 
sub-sumer (LCS) of two synsets is the sumer that does not have any children that 
are also the sub-sumer of the two synsets. In other words, the LCS of two 
synsets is the most specific sub-sumer of the two synsets. Back to above example, 
the LCS of {car, auto..} and {truck..} is {automotive, motor vehicle}, since the 
{automotive, motor vehicle} is more specific than the common sub-sumer {wheeled 
vehicle}.<br>
<br>
<span class="postbody">The path length gives us a simple way to compute 
relatedness distance between two word senses. There are some issues that need to be addressed:<br>
<br>
- It is possible for two synsets from the same part of speech to have no common 
sub-sumer. Since we did not join all the different top nodes of each part of speech taxonomy, a path cannot always be found between two synsets. But if a unique 
root node is being used, then a path will always exist between any two 
noun/verb synsets. <br>
<br>
- </span>Note that multiple inheritance is allowed in WordNet; some synsets 
belong to more than one taxonomy. So if there is more than one path between two 
synsets, the shortest such path is selected. <span class="postbody"><br>
<br>
- Lemmatization : when looking up a word in WN, the word is first lemmatized. 
Therefore, 
the distance between &quot;book&quot; and &quot;books&quot; is 0 since they are identical.&nbsp; But 
&quot;Mice&quot; and &quot;mouse&quot; ?<br>
<br>
- This measurement only compares the word senses which have the same part of 
speech (POS). This means that we do not compare a noun and a verb because they 
are located in different taxonomies. We just consider the words that are nouns, 
verbs, or adjectives respectively.&nbsp; With the omission of the POS tagger, we will use Jeff Martin's 
Lexicon class.&nbsp; When 
considering a word, we first check if it is a noun and if so we will treat it as 
a noun and its verb or adjective will be disregarded.&nbsp; If it is not a noun, we will 
check if it is a verb... <br>
<br>
- Compound nouns like &quot;travel agent&quot; will be treated as two single words 
via the tokenization. <br>
<br>
<b>Measuring similarity (MS1):<br>
</b><br>
There are many proposals for measuring semantic similarity between two synsets: 
Wu &amp; Palmer, Leacock &amp; Chodorow,&nbsp; P.Resnik. In this work, we experimented 
with two simple measurements:</span></p>

<pre lang="text"><span class="postbody">Sim(s, <span lang="en-us">t</span>) = 1/distance(s, <span lang="en-us">t</span>).</span></pre>
<ul>
  <li> <span class="postbody">Where distance is the path length from  </span>
  <code>s</code><span class="postbody"> to  </span><code>t</code><span class="postbody"> using node counting.</span></li>
</ul>

<p> <span class="postbody">
<b>Measuring similarity (MS2):<br>
<br>
</b>This formula was used in the previous article, which not only took into 
account the length of the path but also the </span>the order of the sense 
involved in this path. </p>

<pre lang="text">Sim(s,<span lang="en-us"> </span>t)<span lang="en-us"> </span>=<span lang="en-us"> </span>Sen<span lang="en-us">se</span>Weight(s)*Sen<span lang="en-us">se</span>Weight(t)/PathLength</pre>
<ul>
  <li>Where <code>s</code> and <code>t</code>: denote the source and target 
  words being compared. </li>
  <li><code>SenseWeight</code>: denotes a weight calculated according to the 
  order of this sense and the count of total senses. </li>
  <li><code>PathLength</code>: denotes the length of the connection path from
  <code>s</code> to <code>t</code>. </li>
</ul>

<h2>Semantic similarity between two sentences</h2>

<p>We will now describe the overall strategy to capture semantic similarity between 
two sentences.<span class="postbody"> <br>
Given two 
sentences X and Y,&nbsp; we denote m to be length of X, n to be length of Y.&nbsp;
The major steps can be described as follows:</span></p>

<ol>
  <li>Tokenization.</li>
  <li>Perform word stemming.</li>
  <li>Perform part of speech tagging.</li>
  <li>Word sense disambiguation.</li>
  <li> <span class="postbody"> Building a semantic similarity relative matrix R[m, n] of each pair of 
word senses, where R[i, 
j] is semantic simiarity between the most appropriate sense of word at position 
i of X and the most appropriate sense of word at position j of Y.&nbsp; Thus, 
  R[i,j] is also weight of edge connect from i to j. If a word does 
not exist in the dictionary we use the edit-distance similarity instead and 
  output a lower associated weight;&nbsp; for example : an abbreviation like CTO (Chief of Technology Officer).</span></li>
  <li> <span class="postbody"> We formulate the problem of capturing semantic similarity 
  between sentences as the problem of computing a maximum total matching weight of 
	a bipartite graph, where X and Y are two sets of 
disjoint nodes.&nbsp; We use the Hungarian method to solve this problem; please 
  refer to our previous article [3]. <br>
If computational time is critical, we can use a simple quick heuristic method as 
follows:<br>
  Pseudo- code</span><pre>ScoreSum &lt;- 0;

<b>foreach</b> (X[i] in X){
  bestCandidate &lt;- -1;
  bestScore &lt;- -maxInt;
 <b> foreach</b> (Y[j] in Y){ 
    <b>if</b> (Y[j] is still free &amp;&amp; r[i, j] &gt; bestScore){
        bestScore &lt;- R[i, j]; 
        bestCandidate &lt;- j;		    	
      }  
  }

  <b>if</b> (bestCandidate != -1){
      mark the bestCandidate as matched item.    
      scoreSum &lt;- scoreSum + bestScore;
  }
}
</pre>

  </li>
  <li><span class="postbody">The match results from the previous step are 
combined into a single similarity value for two sentences.</span>  <span class="postbody">There are many strategies to acquire an 
overall combined similarity value for sets of 
matching pairs. In the previous section, we presented two simple&nbsp;formulas 
to compute semantic 
similarity between two word-senses. For each formula we apply an appropriate
strategy to compute the overall score:<br>
&nbsp;</span><ul>
    <li><span class="postbody">Matching average: 
    <img border="0" src="average.gif" width="115" height="63">Where match(X, Y) 
    are the matching word tokens between X and Y. This similarity is computed by dividing the sum 
of similarity values of all match candidates of both sentence X and Y by the 
total number of set tokens.&nbsp; An important point is that it is based on each 
	of the individual similarity values, so that the overall similarity always reflects the 
influence of them. We apply this strategy using the MS1 formula.<br>
&nbsp;</span></li>
    <li>Dice coefficient <img border="0" src="codepr2.gif" width="88" height="59">This 
  strategy returns the ratio of the number of tokens that can be matched over the 
  total of tokens. We apply this strategy using the MS2 formula.&nbsp; Hence, Dice will always return a higher value than Matching average and 
it is thus more optimistic. In this strategy we need to predefine a threshold to 
  select the matching pairs that have values exceeding the given threshold.</li>
    <li>(Cosine, Jarccard, Simpson coefficients will be 
considered in other particular situations).</li>
  </ul>
  <p>For example: <br>
  Given two sentences X and Y, X has length of 3 and Y has 
  length of 2. The bipartite matcher returns that X1 is matched Y1 with score 0.8, X2 is 
  matched Y2 with score 0.7. <br>
  + using Matching average, the overall score is : 2*(0.8 + 0.7) / (3 + 2) = 
  0.6.<br>
  + using Dice the threshold is 0.5, since both matching pairs have scores 
  greater than the threshold, so we have total of 2 matching pairs. &nbsp;-&gt; The overall score is:&nbsp; 
  2*(1 + 1)/ (3+2) = 0.8.</li>
</ol>

<h2>Using the code</h2>

<p>To run this code, you should install WordNet 2.1. </p>

<pre>/
// Any source code blocks look like this
//
</pre>

<h2>Future work</h2>

<p>Time restrictions are a problem; whenever possible we would like to do:<br>
<br>
- Improve the usability of this experiment. <br>
- Extend the WSD algorithm with supervised learning with such methods as the Naive Bayesian Classifier model. <br>
- Disambiguate part of speech using decision trees. </p>

<h2>Conclusion</h2>

<p>So far we have presented a simple approach to capturing semantic similarity.&nbsp;This 
work might have many limitations since we are not a NLP research group.<br>
<br>
There is a Perl open source package for semantic similarity from T.Pederson. 
Unfortunately We do not know Perl; it would be very helpful if someone could migrate 
it to .NET. We'll stop here for now and hope that others might be inspired to work on WordNet.Net to develop this open source 
library to be more useful. <h2>Points of Interest</h2>

<p>&nbsp;</p>

<font FACE="Times New Roman" SIZE="3">

<h2>My acknowledgements</h2>

<p ALIGN="LEFT">Many thanks to:<br>
WordNet Princeton, E.Briller, M.Porter, M.Crowe, T.Pedersen - his team(S.Banerjee, 
J.Michelizzi, S. Patwardhan) , P.Resnik, Hirst - S.T.Onge the NLP research community.&nbsp; <br>
<br>
We would like to thank M.A.Warin, C.Lemon, Richard.N, who had provided helpful document resources and comments during this work.<br>
&nbsp;</p>

</font>

<font SIZE="3">

<h2>References</h2>

</font>

<ol>
  <li>A.V. Goldberg, R. Kennedy: An Efficient cost scaling algorithm for the 
  assignment problem, 1993.</li>
  <li>WordNet by Princeton.</li>
  <li>T.Dao : The longest common sub-string with maximal consecutive, 2005.</li>
  <li>T.Dao : An improvement on capturing similarity between strings, 2005.</li>
  <li>T.Simpson: WordNet.NET Introduction.</li>
  <li>T. Pedersen, S. Banerjee, S. Patwardhan: Maximize semantic relatedness to 
  perform word sense disambiguation, 2005.</li>
  <li>P. Resnik: WordNet and class-based probabilities.</li>
  <li>G. Hirst, D.St. Onge:&nbsp; Lexical chains as representation of context 
  for the detection and correction of malapropisms.</li>
  <li>M. Lesk: Automatic sense disambiguation using machine readable 
  dictionaries: how to tell a pine code from an ice cream cone, 1986.</li>
  <li>Zipf distribution law.</li>
  <li><font FACE="CMR10" SIZE="3">
  <p ALIGN="LEFT">S. Banerjee, T. Pedersen: Extended gloss overlaps as a measure 
  of semantic relatedness, 2003.</font></li>
  <li>Wu, Palmer: Depth of subsuming concept scaled by sum of the depths of 
  individual concepts, 1994.</li>
</ol>
</body>
</html>